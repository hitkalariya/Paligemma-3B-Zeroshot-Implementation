{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================================\n",
    "# PaliGemma: The Engine Behind Our Universal Vision AI\n",
    "# ====================================================\n",
    "<!-- #\n",
    "# The application we've built is powered by **PaliGemma**, a state-of-the-art\n",
    "# open Vision Language Model (VLM) from Google. To understand why it's so\n",
    "# revolutionary and the perfect choice for this project, let's break down\n",
    "# what it is and what makes it special.\n",
    "#\n",
    "# ### What is PaliGemma? The Name is the Formula.\n",
    "#\n",
    "# The name itself tells you everything about its architecture:\n",
    "#\n",
    "# *   **Pali:** Stands for **Pa**thways **L**anguage and **I**mage model. It signifies\n",
    "#     a model that finds a \"path\" to connect visual information with human language.\n",
    "# *   **Gemma:** This is the family of powerful, lightweight, open-source language\n",
    "#     models from Google, derived from the same research and technology used to\n",
    "#     create the Gemini models.\n",
    "#\n",
    "# Essentially, **PaliGemma = A Powerful Vision Component + A Powerful Language Model**.\n",
    "#\n",
    "# ### How Does It Work? The Architecture of a Digital Eye and Brain\n",
    "#\n",
    "# Imagine you have two experts who speak different languages: an expert art critic\n",
    "# who can only see (the **Vision Encoder**) and an expert linguist who can only\n",
    "# read and write (the **Language Decoder**).\n",
    "#\n",
    "# ![PaliGemma Architecture Diagram](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/image1_jH8vT1j.width-1300.png)\n",
    "#\n",
    "# 1.  **The Vision Encoder (The Eye):** This part of PaliGemma is a sophisticated\n",
    "#     **Vision Transformer (ViT)**. It doesn't see an image as just a grid of pixels.\n",
    "#     Instead, it breaks the image down into a series of smaller patches, like a\n",
    "#     jigsaw puzzle. It analyzes each patch and understands its content and its\n",
    "#     relationship to the other patches. It then converts this complex visual\n",
    "#     understanding into a special, compressed digital language.\n",
    "#\n",
    "# 2.  **The Language Decoder (The Brain):** This is the **Gemma** language model.\n",
    "#     It's a master of text, grammar, reasoning, and multiple languages. It can\n",
    "#     write poetry, answer questions, and follow complex instructions, but it can't see.\n",
    "#\n",
    "# 3.  **The Projector (The Universal Translator):** This is the magic component.\n",
    "#     The \"projector\" is a small but crucial neural network that sits between the\n",
    "#     Eye and the Brain. Its only job is to translate the digital language of the\n",
    "#     vision encoder into a format that the Gemma language model can understand\n",
    "#     and reason about.\n",
    "#\n",
    "# When you give our app an image and a prompt like `\"detect the cars\"`, this is what happens:\n",
    "# *   The **Vision Encoder** looks at the image and creates a rich, detailed digital summary.\n",
    "# *   The **Projector** translates this visual summary into \"words\" that the language model can read.\n",
    "# *   The **Language Model** receives a combined instruction: `[visual summary words] + \"detect the cars\"`.\n",
    "# *   Because it now has both the visual context and your command, it can reason about\n",
    "#     the image and generate the precise answer, including the special `<loc...>` tokens\n",
    "#     that pinpoint exactly where the cars are.\n",
    "#\n",
    "# ---\n",
    "#\n",
    "# ### Why is PaliGemma So Amazing? Its Superpowers\n",
    "#\n",
    "# PaliGemma's unique architecture gives it several superpowers that made it the\n",
    "# perfect choice for our application:\n",
    "#\n",
    "# #### 1. True Zero-Shot Versatility\n",
    "# This is its most incredible feature. \"Zero-shot\" means it can perform tasks it was\n",
    "# **never explicitly trained to do**. Because it has a general understanding of both\n",
    "# images and language, you can give it almost any free-form command, and it will\n",
    "# reason its way to the correct answer.\n",
    "#\n",
    "# > **Our App's Benefit:** This is *why* our single interface works for everything.\n",
    "# > We don't need separate models or modes for OCR, VQA, and detection. We just\n",
    "# > pass your natural language prompt (`\"read the text\"`, `\"what color is the car?\"`,\n",
    "# > `\"caption in hindi\"`) to PaliGemma, and it figures out what to do.\n",
    "#\n",
    "# #### 2. Visual Grounding\n",
    "# PaliGemma doesn't just recognize that a \"cow\" is in an image; it knows the\n",
    "# *exact pixel coordinates* of that cow. When you ask it to \"detect a cow,\" it\n",
    "# outputs the label \"a cow\" preceded by special location tokens.\n",
    "#\n",
    "# ```\n",
    "# <loc0274><loc0000><loc1021><loc0700> a cow\n",
    "# ```\n",
    "#\n",
    "# > **Our App's Benefit:** This is what allows our UI to parse those tokens and\n",
    "# > draw the pink bounding boxes directly on the output image, providing a rich,\n",
    "# > visual result that goes beyond simple text.\n",
    "#\n",
    "# #### 3. Innate Multilingualism\n",
    "# PaliGemma was trained on a massive, web-scale dataset containing images and text\n",
    "# from countless languages. This means it has a built-in understanding of languages\n",
    "# beyond English.\n",
    "#\n",
    "# > **Our App's Benefit:** This is why you can ask for a caption in Hindi, Gujarati,\n",
    "# > or Spanish. By providing a strong prompt (as we now do in our backend), we can\n",
    "# > tap into this latent ability and get reliable multilingual output without\n",
    "# > needing a separate translation service.\n",
    "#\n",
    "# #### 4. A Unified, Efficient Architecture\n",
    "# Before models like PaliGemma, building our application would have required\n",
    "# stitching together multiple different AI models: one for OCR, one for object\n",
    "# detection, one for VQA, etc. This would be complex, slow, and expensive.\n",
    "#\n",
    "# > **Our App's Benefit:** PaliGemma provides all of this functionality in a single,\n",
    "# > elegant model. This simplifies our code, reduces resource consumption, and allows\n",
    "# > us to have one powerful, flexible tool that can handle any visual task you\n",
    "# > throw at it.\n",
    "#\n",
    "# In summary, PaliGemma is not just another model; it's a leap forward in creating\n",
    "# general-purpose AI that can see, read, and reason about the world in a way that\n",
    "# is remarkably human-like. It's the perfect engine for the sleek, powerful, and\n",
    "# versatile application we have built together. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-24T02:00:45.548Z",
     "iopub.execute_input": "2025-06-24T01:52:28.302547Z",
     "iopub.status.busy": "2025-06-24T01:52:28.302309Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: Pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
      "Requirement already satisfied: flask>=3.0.3 in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
      "Collecting python-dotenv>=1.0.1\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pyngrok>=7.1.6\n",
      "  Using cached pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Collecting bitsandbytes>=0.43.1\n",
      "  Using cached bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3.0)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3.0)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3.0)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.30.1) (7.0.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.3) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.3) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.3) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.3) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.3) (2025.4.26)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.41.2) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3.0) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.41.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.41.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.41.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.41.2) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.41.2) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.41.2) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.41.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.41.2) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.41.2) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.41.2) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.41.2) (2024.2.0)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-dotenv, pyngrok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "Successfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyngrok-7.2.11 python-dotenv-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 01:54:12.143969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750730052.533283      82 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750730052.632753      82 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:4056: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d403ce86cb4a37a1fb386827398faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a67ba636cb4386a2452e0f197ef428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185d0dc784014ab6b4805254993b1129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41e12c9c304445d894cf4dd70207ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0568942a32a40949aebd6d703945493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5899d6064f1a4563996391ccfa803b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6220775cf9664813aacad93e147b601b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c8f9f9ecd745a8b671a2cc7b8a640c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/processing_auto.py:243: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ae97e300ed4f658efa61c4e0e6c174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7103cd58d1b4997abb609e775aefb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9288ac2af03f4b22b064a05e7bbf9e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b29e2d24ae642f1bf6bf23f384ebe3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae57109f851c4eecbadd41ac7a12459b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f86031c37b46cabd8f980f9305b302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================                    \n",
      "                 ✅ Your Futuristic Vision AI Interface is LIVE!                 \n",
      "              Public URL: https://b2f7-34-123-105-253.ngrok-free.app            \n",
      "================================================================================\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# app.py: Self-Contained Zero-Shot Web App\n",
    "#\n",
    "# Description:\n",
    "# This definitive version features a non-scrolling page layout, a scrollable\n",
    "# output text area, and a significantly enhanced, more impactful particles.js\n",
    "# background, all within a polished, futuristic user interface.\n",
    "#\n",
    "# Author: Gemini AI Architect\n",
    "# Date: June 11, 2025\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. Dependencies (Run this in a separate cell in Kaggle)\n",
    "#\n",
    "!pip install \"transformers>=4.41.2\" \"torch>=2.3.0\" \"accelerate>=0.30.1\" \\\n",
    "             \"Pillow>=10.3.0\" \"flask>=3.0.3\" \"python-dotenv>=1.0.1\" \\\n",
    "             \"pyngrok>=7.1.6\" \"requests>=2.32.3\" \"bitsandbytes>=0.43.1\"\n",
    "# ==============================================================================\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from flask import Flask, jsonify, request\n",
    "from PIL import Image\n",
    "from pyngrok import ngrok\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Environment and Secrets Loading (for Kaggle)\n",
    "# ==============================================================================\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    NGROK_AUTH_TOKEN = user_secrets.get_secret(\"NGROK_AUTH_TOKEN\")\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    os.environ['NGROK_AUTH_TOKEN'] = NGROK_AUTH_TOKEN\n",
    "except ImportError:\n",
    "    print(\"Kaggle secrets not found. Using .env for local development.\")\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    NGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Hugging Face token not found. Please set HF_TOKEN as a Kaggle secret.\")\n",
    "if not NGROK_AUTH_TOKEN:\n",
    "    raise ValueError(\"Ngrok token not found. Please set NGROK_AUTH_TOKEN as a Kaggle secret.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Logging Configuration\n",
    "# ==============================================================================\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Embedded HTML, CSS, and JavaScript (Enhanced Professional UI)\n",
    "# ==============================================================================\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\" data-theme=\"dark\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Paligemma – A Vision Language Model</title>\n",
    "    <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "    <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "    <link href=\"https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&display=swap\" rel=\"stylesheet\">\n",
    "    <style>\n",
    "        :root {\n",
    "            --bg: #121212;\n",
    "            --surface: #1f1f1f;\n",
    "            --panel-border: #2a2a2a;\n",
    "            --primary: #4361ee;\n",
    "            --accent: #04d361;\n",
    "            --highlight: #f72585;\n",
    "            --text: #d9d9d9;\n",
    "            --text-bright: #ffffff;\n",
    "            --shadow: rgba(255, 255, 255, 0.05);\n",
    "            --font-family: 'Poppins', sans-serif;\n",
    "        }\n",
    "        * { box-sizing: border-box; }\n",
    "        html, body { scroll-behavior: smooth; }\n",
    "        body {\n",
    "            font-family: var(--font-family);\n",
    "            background-color: var(--bg);\n",
    "            color: var(--text);\n",
    "            margin: 0;\n",
    "            transition: background-color 0.3s ease, color 0.3s ease;\n",
    "            overflow-x: hidden;\n",
    "        }\n",
    "        #particles-js {\n",
    "            position: fixed; top: 0; left: 0; width: 100%; height: 100%;\n",
    "            pointer-events: none; z-index: -1;\n",
    "        }\n",
    "        .app { display: flex; flex-direction: column; min-height: 100vh; }\n",
    "        header { text-align: center; padding: 40px 20px; position: relative; }\n",
    "        .main-title {\n",
    "            font-size: clamp(2.2rem, 5vw, 3.5rem);\n",
    "            font-weight: 700;\n",
    "            color: var(--text-bright);\n",
    "            position: relative;\n",
    "            display: inline-block;\n",
    "            padding-bottom: 10px;\n",
    "        }\n",
    "        .main-title::after {\n",
    "            content: '';\n",
    "            position: absolute;\n",
    "            width: 100%;\n",
    "            height: 4px;\n",
    "            bottom: 0;\n",
    "            left: 0;\n",
    "            background: linear-gradient(90deg, var(--accent), var(--primary), var(--highlight), var(--accent));\n",
    "            background-size: 200% 100%;\n",
    "            animation: run-glow 4s linear infinite;\n",
    "        }\n",
    "        main { flex-grow: 1; padding: 0 20px 40px 20px; }\n",
    "        .grid {\n",
    "            display: grid; grid-template-columns: 1fr 1fr; gap: 30px;\n",
    "            align-items: start; max-width: 1400px; margin: 0 auto;\n",
    "        }\n",
    "        @media (max-width: 960px) { .grid { grid-template-columns: 1fr; } .output-panel { margin-top: 30px; } }\n",
    "        .input-panel, .output-panel {\n",
    "            background: var(--surface);\n",
    "            border: 1px solid var(--panel-border);\n",
    "            border-radius: 16px;\n",
    "            padding: 25px;\n",
    "            transition: border-color .3s, box-shadow .3s;\n",
    "            box-shadow: 0 10px 30px rgba(0,0,0,0.2);\n",
    "        }\n",
    "        .input-panel.glow, .output-panel.glow {\n",
    "            animation: pulse-glow 1.5s ease-out;\n",
    "        }\n",
    "        @keyframes pulse-glow {\n",
    "            0% { border-color: var(--panel-border); box-shadow: 0 10px 30px rgba(0,0,0,0.2); }\n",
    "            50% { border-color: var(--primary); box-shadow: 0 0 25px var(--primary); }\n",
    "            100% { border-color: var(--panel-border); box-shadow: 0 10px 30px rgba(0,0,0,0.2); }\n",
    "        }\n",
    "        .input-controls { display: flex; flex-direction: column; gap: 15px; margin-bottom: 15px; }\n",
    "        .button {\n",
    "            background: var(--bg);\n",
    "            color: var(--text); border: 1px solid var(--shadow); padding: 12px;\n",
    "            border-radius: 8px; font-size: 1rem; font-weight: 500; cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "        }\n",
    "        .button:hover { border-color: var(--primary); color: var(--text-bright); transform: translateY(-2px); box-shadow: 0 4px 10px rgba(0,0,0,0.2); }\n",
    "        .button:active { transform: translateY(-1px) scale(0.98); }\n",
    "        #submit-btn {\n",
    "            background: linear-gradient(90deg, var(--primary) 0%, var(--highlight) 100%);\n",
    "            color: white; font-weight: 600;\n",
    "            display: flex; align-items: center; justify-content: center; gap: 10px;\n",
    "        }\n",
    "        .url-input, textarea {\n",
    "            background: var(--bg); border: 1px solid var(--shadow);\n",
    "            color: var(--text); border-radius: 8px; padding: 12px;\n",
    "            font-family: var(--font-family); font-size: 1rem; width: 100%;\n",
    "            transition: all 0.3s ease;\n",
    "        }\n",
    "        .url-input:focus, textarea:focus {\n",
    "            border-color: var(--primary);\n",
    "            box-shadow: 0 0 12px rgba(67, 97, 238, 0.5);\n",
    "            outline: none;\n",
    "        }\n",
    "        textarea { min-height: 120px; resize: vertical; }\n",
    "        .image-box {\n",
    "            background: var(--bg); border: 1px solid var(--shadow);\n",
    "            height: 350px; /* Fixed height */\n",
    "            display: flex; align-items: center; justify-content: center;\n",
    "            border-radius: 8px; position: relative; overflow: hidden;\n",
    "            transition: min-height 0.3s ease;\n",
    "        }\n",
    "        .image-box img {\n",
    "            max-width: 100%; max-height: 100%; object-fit: contain;\n",
    "            animation: pop-in 0.5s cubic-bezier(0.25, 0.8, 0.25, 1) forwards;\n",
    "        }\n",
    "        @keyframes pop-in {\n",
    "            from { transform: scale(0.95); opacity: 0; }\n",
    "            to { transform: scale(1); opacity: 1; }\n",
    "        }\n",
    "        .output-text {\n",
    "            background: var(--bg); border-left: 4px solid var(--highlight);\n",
    "            padding: 16px; color: var(--text-bright);\n",
    "            margin-top: 20px; font-family: monospace; font-size: 18px; line-height: 1.6;\n",
    "            border-radius: 0 8px 8px 0; white-space: pre-wrap; word-wrap: break-word;\n",
    "            animation: fade-slide-in 0.5s ease-out forwards;\n",
    "            /* *** CRITICAL FIX FOR SCROLLING *** */\n",
    "            max-height: 400px;\n",
    "            overflow-y: auto;\n",
    "        }\n",
    "        @keyframes fade-slide-in {\n",
    "            from { opacity: 0; transform: translateY(10px); }\n",
    "            to { opacity: 1; transform: translateY(0); }\n",
    "        }\n",
    "        footer { text-align: center; padding: 40px 20px; }\n",
    "        .footer-credit {\n",
    "            font-size: 32px; color: var(--text);\n",
    "            text-decoration: none; position: relative; display: inline-block;\n",
    "        }\n",
    "        .footer-credit::after {\n",
    "            content: ''; position: absolute; width: 100%; height: 3px;\n",
    "            bottom: -5px; left: 0;\n",
    "            background: linear-gradient(90deg, var(--accent), var(--primary), var(--highlight), var(--accent));\n",
    "            background-size: 200% 100%;\n",
    "            animation: run-glow 4s linear infinite;\n",
    "        }\n",
    "        @keyframes run-glow { to { background-position: -200% 0; } }\n",
    "        .placeholder-text { color: var(--text); opacity: 0.5; }\n",
    "        .bounding-box { position: absolute; box-sizing: border-box; border: 2px solid var(--highlight); background-color: rgba(247, 37, 133, 0.2); }\n",
    "        .box-label { position: absolute; top: -22px; left: -2px; background-color: var(--highlight); color: white; padding: 2px 6px; font-size: 12px; font-weight: 600; border-radius: 4px; }\n",
    "        .spinner {\n",
    "            width: 20px; height: 20px; border-radius: 50%;\n",
    "            border: 3px solid rgba(255,255,255,0.2);\n",
    "            border-top-color: #ffffff;\n",
    "            animation: spin 1s linear infinite;\n",
    "        }\n",
    "        @keyframes spin { to { transform: rotate(360deg); } }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"particles-js\"></div>\n",
    "    <div class=\"app\">\n",
    "        <header>\n",
    "            <h1 class=\"main-title\">Paligemma – A Vision Language Model</h1>\n",
    "        </header>\n",
    "        <main class=\"grid\">\n",
    "            <section class=\"input-panel\" id=\"input-panel\">\n",
    "                <div class=\"image-box\" id=\"input-image-container\">\n",
    "                    <p class=\"placeholder-text\">Your image will appear here.</p>\n",
    "                    <img id=\"input-image-preview\" src=\"\" alt=\"Input Preview\" style=\"display:none;\">\n",
    "                </div>\n",
    "                <div class=\"input-controls\">\n",
    "                    <button id=\"upload-btn\" class=\"button\">Upload from computer</button>\n",
    "                    <input type=\"file\" id=\"file-upload\" hidden>\n",
    "                </div>\n",
    "                <input type=\"text\" id=\"image-url\" class=\"url-input\" placeholder=\"Paste image URL…\">\n",
    "                <textarea id=\"task-prompt\" placeholder=\"Type your AI query or command…\"></textarea>\n",
    "                <button id=\"submit-btn\" class=\"button\">\n",
    "                    <span id=\"btn-text\">Analyze Image</span>\n",
    "                    <div class=\"spinner\" id=\"spinner\" style=\"display: none;\"></div>\n",
    "                </button>\n",
    "            </section>\n",
    "            <section class=\"output-panel\" id=\"output-panel\">\n",
    "                <div class=\"image-box\" id=\"output-image-container\">\n",
    "                    <p class=\"placeholder-text\">Your processed image will appear here.</p>\n",
    "                    <img id=\"output-image\" src=\"\" alt=\"Result Image\" style=\"display:none;\">\n",
    "                </div>\n",
    "                <pre class=\"output-text\" id=\"output-text-container\" style=\"display:none;\"></pre>\n",
    "                <div id=\"error-display\" style=\"color: var(--highlight); display:none; margin-top: 15px;\"></div>\n",
    "            </section>\n",
    "        </main>\n",
    "        <footer>\n",
    "            <a href=\"#\" class=\"footer-credit\">Made by Hit Kalariya</a>\n",
    "        </footer>\n",
    "    </div>\n",
    "    \n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/particles.js@2.0.0/particles.min.js\"></script>\n",
    "    <script>\n",
    "    document.addEventListener('DOMContentLoaded', () => {\n",
    "        // --- Particles.js Initialization (Enhanced Version) ---\n",
    "        particlesJS('particles-js', {\n",
    "            \"particles\": {\n",
    "                \"number\": {\"value\": 80, \"density\": {\"enable\": true, \"value_area\": 800}},\n",
    "                \"color\": {\"value\": \"#4361ee\"},\n",
    "                \"shape\": {\"type\": \"circle\"},\n",
    "                \"opacity\": {\"value\": 0.5, \"random\": true, \"anim\": {\"enable\": true, \"speed\": 1, \"opacity_min\": 0.1, \"sync\": false}},\n",
    "                \"size\": {\"value\": 6, \"random\": true, \"anim\": {\"enable\": true, \"speed\": 3, \"size_min\": 1, \"sync\": false}},\n",
    "                \"line_linked\": {\"enable\": true, \"distance\": 150, \"color\": \"#f72585\", \"opacity\": 0.3, \"width\": 3},\n",
    "                \"move\": {\"enable\": true, \"speed\": 0.5, \"direction\": \"none\", \"random\": true, \"straight\": false, \"out_mode\": \"out\"}\n",
    "            },\n",
    "            \"interactivity\": {\n",
    "                \"detect_on\": \"canvas\",\n",
    "                \"events\": {\"onhover\": {\"enable\": true, \"mode\": \"grab\"}, \"resize\": true},\n",
    "                \"modes\": {\"grab\": {\"distance\": 200, \"line_linked\": {\"opacity\": 0.5}}}\n",
    "            },\n",
    "            \"retina_detect\": true\n",
    "        });\n",
    "\n",
    "        // --- DOM Element Selection ---\n",
    "        const uploadBtn = document.getElementById('upload-btn');\n",
    "        const fileUpload = document.getElementById('file-upload');\n",
    "        const imageUrlInput = document.getElementById('image-url');\n",
    "        const taskPrompt = document.getElementById('task-prompt');\n",
    "        const submitBtn = document.getElementById('submit-btn');\n",
    "        const btnText = document.getElementById('btn-text');\n",
    "        const spinner = document.getElementById('spinner');\n",
    "        const inputImageContainer = document.getElementById('input-image-container');\n",
    "        const inputImagePreview = document.getElementById('input-image-preview');\n",
    "        const outputImageContainer = document.getElementById('output-image-container');\n",
    "        const outputImage = document.getElementById('output-image');\n",
    "        const outputTextContainer = document.getElementById('output-text-container');\n",
    "        const errorDisplay = document.getElementById('error-display');\n",
    "        const inputPanel = document.getElementById('input-panel');\n",
    "        const outputPanel = document.getElementById('output-panel');\n",
    "        let currentImageSource = null;\n",
    "        let isInferring = false;\n",
    "\n",
    "        // --- Bounding Box & State Management ---\n",
    "        function clearBoundingBoxes() {\n",
    "            outputImageContainer.querySelectorAll('.bounding-box').forEach(box => box.remove());\n",
    "        }\n",
    "        function drawBoundingBoxes(detections) {\n",
    "            clearBoundingBoxes();\n",
    "            if (!detections || !Array.isArray(detections)) return;\n",
    "            const containerWidth = outputImageContainer.offsetWidth;\n",
    "            const containerHeight = outputImageContainer.offsetHeight;\n",
    "            const imageNaturalWidth = outputImage.naturalWidth;\n",
    "            const imageNaturalHeight = outputImage.naturalHeight;\n",
    "            if (imageNaturalWidth === 0 || imageNaturalHeight === 0) return;\n",
    "            const imageAspectRatio = imageNaturalWidth / imageNaturalHeight;\n",
    "            const containerAspectRatio = containerWidth / containerHeight;\n",
    "            let scaledWidth, scaledHeight, offsetX, offsetY;\n",
    "            if (imageAspectRatio > containerAspectRatio) {\n",
    "                scaledWidth = containerWidth;\n",
    "                scaledHeight = scaledWidth / imageAspectRatio;\n",
    "                offsetX = 0;\n",
    "                offsetY = (containerHeight - scaledHeight) / 2;\n",
    "            } else {\n",
    "                scaledHeight = containerHeight;\n",
    "                scaledWidth = scaledHeight * imageAspectRatio;\n",
    "                offsetY = 0;\n",
    "                offsetX = (containerWidth - scaledWidth) / 2;\n",
    "            }\n",
    "            detections.forEach(detection => {\n",
    "                const { box, label } = detection;\n",
    "                const [x_min_pct, y_min_pct, x_max_pct, y_max_pct] = box;\n",
    "                const boxDiv = document.createElement('div');\n",
    "                boxDiv.className = 'bounding-box';\n",
    "                boxDiv.style.left = `${(x_min_pct * scaledWidth) + offsetX}px`;\n",
    "                boxDiv.style.top = `${(y_min_pct * scaledHeight) + offsetY}px`;\n",
    "                boxDiv.style.width = `${(x_max_pct - x_min_pct) * scaledWidth}px`;\n",
    "                boxDiv.style.height = `${(y_max_pct - y_min_pct) * scaledHeight}px`;\n",
    "                const labelSpan = document.createElement('span');\n",
    "                labelSpan.className = 'box-label';\n",
    "                labelSpan.innerText = label;\n",
    "                boxDiv.appendChild(labelSpan);\n",
    "                outputImageContainer.appendChild(boxDiv);\n",
    "            });\n",
    "        }\n",
    "        \n",
    "        function triggerGlow(element) {\n",
    "            element.classList.remove('glow');\n",
    "            void element.offsetWidth;\n",
    "            element.classList.add('glow');\n",
    "        }\n",
    "\n",
    "        function handleImageInput(src) {\n",
    "            currentImageSource = src;\n",
    "            inputImagePreview.src = src;\n",
    "            inputImagePreview.style.display = 'block';\n",
    "            inputImageContainer.querySelector('.placeholder-text').style.display = 'none';\n",
    "            triggerGlow(inputPanel);\n",
    "        }\n",
    "\n",
    "        // --- Event Handlers ---\n",
    "        uploadBtn.addEventListener('click', () => fileUpload.click());\n",
    "        fileUpload.addEventListener('change', (e) => {\n",
    "            const file = e.target.files[0];\n",
    "            if (file) {\n",
    "                const reader = new FileReader();\n",
    "                reader.onload = (event) => {\n",
    "                    handleImageInput(event.target.result);\n",
    "                    imageUrlInput.value = '';\n",
    "                };\n",
    "                reader.readAsDataURL(file);\n",
    "            }\n",
    "        });\n",
    "        imageUrlInput.addEventListener('input', () => {\n",
    "            if (imageUrlInput.value) {\n",
    "                handleImageInput(imageUrlInput.value);\n",
    "                fileUpload.value = null;\n",
    "            }\n",
    "        });\n",
    "        \n",
    "        // --- Main Inference Function ---\n",
    "        async function triggerInference() {\n",
    "            if (!currentImageSource || !taskPrompt.value || isInferring) {\n",
    "                if (!currentImageSource) alert('Please provide an image first.');\n",
    "                if (!taskPrompt.value) alert('Please enter a prompt.');\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            isInferring = true;\n",
    "            btnText.style.display = 'none';\n",
    "            spinner.style.display = 'block';\n",
    "            submitBtn.disabled = true;\n",
    "\n",
    "            clearBoundingBoxes();\n",
    "            errorDisplay.style.display = 'none';\n",
    "            \n",
    "            outputImage.src = currentImageSource;\n",
    "            outputImage.style.display = 'block';\n",
    "            outputImageContainer.querySelector('.placeholder-text').style.display = 'none';\n",
    "            outputTextContainer.style.display = 'none';\n",
    "            \n",
    "            try {\n",
    "                const response = await fetch('/infer', {\n",
    "                    method: 'POST',\n",
    "                    headers: { 'Content-Type': 'application/json' },\n",
    "                    body: JSON.stringify({ image_source: currentImageSource, task: taskPrompt.value }),\n",
    "                });\n",
    "                const data = await response.json();\n",
    "                if (!response.ok) { throw new Error(data.error || 'An unknown error occurred.'); }\n",
    "                \n",
    "                outputTextContainer.textContent = data.result;\n",
    "                outputTextContainer.style.display = 'block';\n",
    "                triggerGlow(outputPanel);\n",
    "\n",
    "                if (data.detections && data.detections.length > 0) {\n",
    "                    outputImage.onload = () => drawBoundingBoxes(data.detections);\n",
    "                    if (outputImage.complete) drawBoundingBoxes(data.detections);\n",
    "                }\n",
    "            } catch (err) {\n",
    "                errorDisplay.textContent = `Error: ${err.message}`;\n",
    "                errorDisplay.style.display = 'block';\n",
    "                outputImage.style.display = 'none';\n",
    "                outputImageContainer.querySelector('.placeholder-text').style.display = 'block';\n",
    "            } finally {\n",
    "                isInferring = false;\n",
    "                btnText.style.display = 'inline';\n",
    "                spinner.style.display = 'none';\n",
    "                submitBtn.disabled = false;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        submitBtn.addEventListener('click', triggerInference);\n",
    "        taskPrompt.addEventListener('keydown', (e) => {\n",
    "            if (e.key === 'Enter' && !e.shiftKey) {\n",
    "                e.preventDefault();\n",
    "                triggerInference();\n",
    "            }\n",
    "        });\n",
    "    });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Model and Processor Initialization\n",
    "# ==============================================================================\n",
    "MODEL_ID = \"google/paligemma-3b-mix-448\"\n",
    "model = None\n",
    "processor = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def initialize_model_and_processor():\n",
    "    \"\"\"Initializes and returns the model and processor.\"\"\"\n",
    "    global model, processor\n",
    "    if model is not None and processor is not None: return\n",
    "    logging.info(f\"Initializing model: {MODEL_ID}...\")\n",
    "    try:\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            MODEL_ID, torch_dtype=torch.bfloat16, device_map=\"auto\", use_auth_token=HF_TOKEN\n",
    "        )\n",
    "        processor = AutoProcessor.from_pretrained(MODEL_ID, use_auth_token=HF_TOKEN)\n",
    "        logging.info(\"Model and processor initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize model/processor: {e}\")\n",
    "        raise\n",
    "\n",
    "initialize_model_and_processor()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Tunnel and Deployment Setup\n",
    "# ==============================================================================\n",
    "try:\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    public_url = ngrok.connect(5000).public_url\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to start ngrok tunnel: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ Your Futuristic Vision AI Interface is LIVE!\".center(80))\n",
    "print(f\"   Public URL: {public_url}\".center(80))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Flask Application with Bounding Box Parsing & Base64 Support\n",
    "# ==============================================================================\n",
    "app = Flask(__name__)\n",
    "\n",
    "def parse_detection_output(text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Parses the model's text output to extract bounding boxes and labels.\"\"\"\n",
    "    detections = []\n",
    "    pattern = re.compile(r\"(<loc\\d{4}><loc\\d{4}><loc\\d{4}><loc\\d{4}>)\\s*([\\w\\s]+)\")\n",
    "    \n",
    "    for part in text.split(';'):\n",
    "        match = pattern.search(part.strip())\n",
    "        if not match: continue\n",
    "        loc_tokens, label = match.groups()\n",
    "        coords = [int(c) for c in re.findall(r'(\\d{4})', loc_tokens)]\n",
    "        if len(coords) != 4: continue\n",
    "        y_min, x_min, y_max, x_max = [c / 1024.0 for c in coords]\n",
    "        detections.append({\"box\": [x_min, y_min, x_max, y_max], \"label\": label})\n",
    "    return detections\n",
    "\n",
    "def resolve_task(user_prompt: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"Maps a free-form user prompt to a structured pipeline task.\"\"\"\n",
    "    prompt_lower = user_prompt.lower()\n",
    "    caption_match = re.search(r\"(?:caption|describe)(?: this)?(?: in ([\\w-]+))?\", prompt_lower)\n",
    "    if caption_match:\n",
    "        language = caption_match.group(1) if caption_match.group(1) else \"en\"\n",
    "        return f\"caption {language}\", {}\n",
    "    detect_match = re.search(r\"detect (.+)\", prompt_lower)\n",
    "    if detect_match:\n",
    "        labels_str = detect_match.group(1)\n",
    "        labels = [label.strip() for label in re.sub(r\"\\band\\b|\\bor\\b\", \";\", labels_str).split(\";\")]\n",
    "        return f\"detect {'; '.join(labels)}\", {\"is_detection\": True}\n",
    "    if any(keyword in prompt_lower for keyword in [\"read\", \"ocr\", \"text\"]):\n",
    "        return \"ocr\", {}\n",
    "    return user_prompt, {\"question\": user_prompt}\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def home():\n",
    "    \"\"\"Serves the main HTML user interface.\"\"\"\n",
    "    return HTML_TEMPLATE\n",
    "\n",
    "@app.route(\"/infer\", methods=[\"POST\"])\n",
    "def infer():\n",
    "    \"\"\"The main inference endpoint supporting URLs and Base64 uploads.\"\"\"\n",
    "    data = request.get_json()\n",
    "    image_source = data.get(\"image_source\")\n",
    "    user_task = data.get(\"task\")\n",
    "\n",
    "    if not image_source or not user_task:\n",
    "        return jsonify({\"error\": \"Missing image or task\"}), 400\n",
    "\n",
    "    try:\n",
    "        if image_source.startswith(\"http\"):\n",
    "            response = requests.get(image_source, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        elif image_source.startswith(\"data:image\"):\n",
    "            header, encoded = image_source.split(\",\", 1)\n",
    "            decoded_bytes = base64.b64decode(encoded)\n",
    "            image = Image.open(BytesIO(decoded_bytes)).convert(\"RGB\")\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Invalid image source format\"}), 400\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Failed to load image: {str(e)}\"}), 400\n",
    "\n",
    "    resolved_task, task_info = resolve_task(user_task)\n",
    "\n",
    "    try:\n",
    "        inputs = processor(text=resolved_task, images=image, return_tensors=\"pt\").to(model.device)\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "        result_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "        clean_result = result_text.split(resolved_task, 1)[-1].strip()\n",
    "\n",
    "        response_data = {\n",
    "            \"result\": clean_result if clean_result else \"Model returned an empty response.\"\n",
    "        }\n",
    "        \n",
    "        if task_info.get(\"is_detection\"):\n",
    "            response_data[\"detections\"] = parse_detection_output(clean_result)\n",
    "\n",
    "        return jsonify(response_data), 200\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Direct model inference failed for task '{user_task}': {e}\")\n",
    "        return jsonify({\"error\": f\"Model inference failed: {str(e)}\"}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
